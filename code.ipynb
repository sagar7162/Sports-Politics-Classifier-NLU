{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ed1489a",
   "metadata": {},
   "source": [
    "# Sport vs Politics Text Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f74538",
   "metadata": {},
   "source": [
    "## 1. Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2a8fc899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import time\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "SEED = 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e4def9",
   "metadata": {},
   "source": [
    "Politics News "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e8381066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already scraped, skipping...\n",
      "\n",
      "Saved 0 headlines to:\n",
      "toi_politics\\headlines_politics.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "output_folder = \"toi_politics\"\n",
    "output_file = os.path.join(output_folder, \"headlines_politics.csv\")\n",
    "\n",
    "def scrape_politics_headlines(page):\n",
    "    base = \"https://timesofindia.indiatimes.com/politics/news\"\n",
    "    url = base if page == 1 else f\"{base}/{page}\"\n",
    "\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    headlines = []\n",
    "    for a in soup.find_all(\"a\"):\n",
    "        href = a.get(\"href\", \"\")\n",
    "        txt  = a.get_text(strip=True)\n",
    "\n",
    "        if txt and \"/politics/news\" in href:\n",
    "            headlines.append(txt)\n",
    "\n",
    "    return headlines\n",
    "\n",
    "# Scrape multiple pages\n",
    "all_headlines = []\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(output_file):\n",
    "    for page_num in range(1, 30):\n",
    "        try:\n",
    "            headlines = scrape_politics_headlines(page_num)\n",
    "            print(f\"Scraped Page {page_num} — {len(headlines)} items\")\n",
    "            for h in headlines:\n",
    "                all_headlines.append([h, \"politics\"])\n",
    "        except Exception as e:\n",
    "            print(\"Error on page\", page_num, e)\n",
    "\n",
    "    # Save to CSV\n",
    "    with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"headline\", \"category\"])\n",
    "        writer.writerows(all_headlines)\n",
    "        \n",
    "else:\n",
    "    print(\"Already scraped, skipping...\")\n",
    "\n",
    "print(f\"\\nSaved {len(all_headlines)} headlines to:\\n{output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c452973",
   "metadata": {},
   "source": [
    "Sports News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a55619e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already scraped, skipping...\n",
      "\n",
      "Saved 0 headlines to:\n",
      "indianexpress_sports\\sports_headlines.csv\n"
     ]
    }
   ],
   "source": [
    "output_folder = \"indianexpress_sports\"\n",
    "output_file = os.path.join(output_folder, \"sports_headlines.csv\")\n",
    "\n",
    "def scrape_sports_headlines(page):\n",
    "    if page == 1:\n",
    "        url = \"https://indianexpress.com/section/sports/\"\n",
    "    else:\n",
    "        url = f\"https://indianexpress.com/section/sports/page/{page}/\"\n",
    "\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    resp = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    headlines = []\n",
    "    for tag in soup.find_all([\"h2\",\"h3\"]):\n",
    "        a = tag.find(\"a\")\n",
    "        if a and a.get_text(strip=True):\n",
    "            headlines.append(a.get_text(strip=True))\n",
    "    return headlines\n",
    "\n",
    "all_headlines = []\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(output_file):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    for i in range(1, 60):\n",
    "        try:\n",
    "            items = scrape_sports_headlines(i)\n",
    "            print(f\"Page {i}: {len(items)} headlines\")\n",
    "            for h in items:\n",
    "                all_headlines.append([h, \"sports\"])\n",
    "        except Exception as e:\n",
    "            print(\"Error on page\", i, e)\n",
    "\n",
    "    # Save to CSV\n",
    "    with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"headline\", \"category\"])\n",
    "        writer.writerows(all_headlines)\n",
    "else:            \n",
    "    print(\"Already scraped, skipping...\")\n",
    "\n",
    "print(f\"\\nSaved {len(all_headlines)} headlines to:\\n{output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe9ab66",
   "metadata": {},
   "source": [
    "## 2. Load the scraped dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "78b87b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw rows: 3135\n",
      "category\n",
      "politics    1660\n",
      "sports      1475\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>India Women vs Australia Women 1st T20I ​Live Cricket Score: IND to take AUS in Sydney</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When Shoaib Malik made MS Dhoni wait for a photoshoot; and why Sehwag and Uthappa were chosen for a tie-breaking bowl-out</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Colombo weather update today, India vs Pakistan T20 World Cup 2026: Will rain affect IND-PAK match today?</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sindhu, Srikanth, and the All England question: Gopichand on its relevance and India’s hopes</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>India vs Pakistan Today Match Playing 11, T20 World Cup 2026: Will Kuldeep Yadav get nod for IND vs PAK in Colombo?</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                    headline  \\\n",
       "0                                     India Women vs Australia Women 1st T20I ​Live Cricket Score: IND to take AUS in Sydney   \n",
       "1  When Shoaib Malik made MS Dhoni wait for a photoshoot; and why Sehwag and Uthappa were chosen for a tie-breaking bowl-out   \n",
       "2                  Colombo weather update today, India vs Pakistan T20 World Cup 2026: Will rain affect IND-PAK match today?   \n",
       "3                               Sindhu, Srikanth, and the All England question: Gopichand on its relevance and India’s hopes   \n",
       "4        India vs Pakistan Today Match Playing 11, T20 World Cup 2026: Will Kuldeep Yadav get nod for IND vs PAK in Colombo?   \n",
       "\n",
       "  category  \n",
       "0   sports  \n",
       "1   sports  \n",
       "2   sports  \n",
       "3   sports  \n",
       "4   sports  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sports = pd.read_csv('indianexpress_sports/sports_headlines.csv')\n",
    "politics = pd.read_csv('toi_politics/headlines_politics.csv')\n",
    "\n",
    "sports = sports[['headline', 'category']].copy()\n",
    "politics = politics[['headline', 'category']].copy()\n",
    "\n",
    "sports['category'] = 'sports'\n",
    "politics['category'] = 'politics'\n",
    "\n",
    "df = pd.concat([sports, politics], ignore_index=True)\n",
    "df['headline'] = df['headline'].fillna('').astype(str)\n",
    "\n",
    "print('Raw rows:', len(df))\n",
    "print(df['category'].value_counts())\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536553a9",
   "metadata": {},
   "source": [
    "## 3. Normalization, tokenization, and n-gram generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "514a8ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_PATTERN = re.compile(r\"[a-z0-9]+(?:'[a-z0-9]+)?\")\n",
    "\n",
    "STOPWORDS = {\n",
    "    'a', 'an', 'the', 'and', 'or', 'for', 'of', 'to', 'in', 'on', 'at', 'by', 'from',\n",
    "    'with', 'as', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'it', 'its',\n",
    "    'that', 'this', 'these', 'those', 'into', 'about', 'over', 'under', 'after',\n",
    "    'before', 'than', 'then', 'their', 'his', 'her', 'they', 'them', 'you', 'your',\n",
    "    'i', 'we', 'our'\n",
    "}\n",
    "\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    replacements = {\n",
    "        '???': \"'\",\n",
    "        '???': \"'\",\n",
    "        '???': '\"',\n",
    "        '??': '\"',\n",
    "        '???': '-',\n",
    "        '???': '-',\n",
    "        '?': ' ',\n",
    "        '???': '...',\n",
    "        ' ': ' ',\n",
    "    }\n",
    "    out = unicodedata.normalize('NFKC', text or '')\n",
    "    for bad, good in replacements.items():\n",
    "        out = out.replace(bad, good)\n",
    "    return out.lower().strip()\n",
    "\n",
    "\n",
    "def tokenize(text: str):\n",
    "    tokens = TOKEN_PATTERN.findall(normalize_text(text))\n",
    "    return [tok for tok in tokens if tok not in STOPWORDS and len(tok) > 1]\n",
    "\n",
    "\n",
    "def build_ngrams(tokens, ngram_range=(1, 1)):\n",
    "    lo, hi = ngram_range\n",
    "    feats = []\n",
    "    for n in range(lo, hi + 1):\n",
    "        if len(tokens) < n:\n",
    "            continue\n",
    "        if n == 1:\n",
    "            feats.extend(tokens)\n",
    "            continue\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            feats.append('_'.join(tokens[i:i + n]))\n",
    "    return feats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bf3391",
   "metadata": {},
   "source": [
    "## 4. Custom feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a9f35ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchVectorizer:\n",
    "    def __init__(self, ngram_range=(1, 1), min_df=2, max_features=12000, use_idf=False, sublinear_tf=False):\n",
    "        self.ngram_range = ngram_range\n",
    "        self.min_df = min_df\n",
    "        self.max_features = max_features\n",
    "        self.use_idf = use_idf\n",
    "        self.sublinear_tf = sublinear_tf\n",
    "        self.vocabulary_ = {}\n",
    "        self.feature_names_ = []\n",
    "        self.idf_ = None\n",
    "\n",
    "    def _doc_terms(self, text):\n",
    "        tokens = tokenize(text)\n",
    "        return build_ngrams(tokens, self.ngram_range)\n",
    "\n",
    "    def fit(self, texts):\n",
    "        doc_freq = Counter()\n",
    "        for text in texts:\n",
    "            terms = self._doc_terms(text)\n",
    "            if not terms:\n",
    "                continue\n",
    "            doc_freq.update(set(terms))\n",
    "\n",
    "        kept = [(term, df) for term, df in doc_freq.items() if df >= self.min_df]\n",
    "        kept.sort(key=lambda x: (-x[1], x[0]))\n",
    "        if self.max_features:\n",
    "            kept = kept[:self.max_features]\n",
    "\n",
    "        self.feature_names_ = [term for term, _ in kept]\n",
    "        self.vocabulary_ = {term: i for i, term in enumerate(self.feature_names_)}\n",
    "\n",
    "        if self.use_idf:\n",
    "            n_docs = max(1, len(texts))\n",
    "            idf_vals = np.ones(len(self.feature_names_), dtype=np.float64)\n",
    "            for term, i in self.vocabulary_.items():\n",
    "                df = doc_freq[term]\n",
    "                idf_vals[i] = math.log((1 + n_docs) / (1 + df)) + 1.0\n",
    "            self.idf_ = idf_vals\n",
    "        else:\n",
    "            self.idf_ = None\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, texts):\n",
    "        row_ind, col_ind, data = [], [], []\n",
    "\n",
    "        for row, text in enumerate(texts):\n",
    "            term_counts = Counter(self._doc_terms(text))\n",
    "            if not term_counts:\n",
    "                continue\n",
    "\n",
    "            for term, count in term_counts.items():\n",
    "                col = self.vocabulary_.get(term)\n",
    "                if col is None:\n",
    "                    continue\n",
    "\n",
    "                val = float(count)\n",
    "                if self.sublinear_tf and count > 0:\n",
    "                    val = 1.0 + math.log(count)\n",
    "                if self.idf_ is not None:\n",
    "                    val *= float(self.idf_[col])\n",
    "\n",
    "                row_ind.append(row)\n",
    "                col_ind.append(col)\n",
    "                data.append(val)\n",
    "\n",
    "        return csr_matrix(\n",
    "            (np.array(data, dtype=np.float64), (np.array(row_ind), np.array(col_ind))),\n",
    "            shape=(len(texts), len(self.vocabulary_)),\n",
    "            dtype=np.float64,\n",
    "        )\n",
    "\n",
    "    def fit_transform(self, texts):\n",
    "        return self.fit(texts).transform(texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25e1e0d",
   "metadata": {},
   "source": [
    "## 5. Clean dataset and make train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "84fa0f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after cleaning: 2893\n",
      "Rows removed: 242\n",
      "category\n",
      "politics    1462\n",
      "sports      1431\n",
      "Name: count, dtype: int64\n",
      "Token length summary by class:\n",
      "            count       mean   50%   max\n",
      "category                               \n",
      "politics  1462.0  10.515048  10.0  21.0\n",
      "sports    1431.0  14.007687  14.0  26.0\n",
      "\n",
      "Train size: 2314 Test size: 579\n"
     ]
    }
   ],
   "source": [
    "df['clean_text'] = df['headline'].map(normalize_text)\n",
    "\n",
    "rows_before = len(df)\n",
    "df = df[df['clean_text'].str.len() > 0].drop_duplicates(subset=['clean_text', 'category']).reset_index(drop=True)\n",
    "rows_after = len(df)\n",
    "\n",
    "print('Rows after cleaning:', rows_after)\n",
    "print('Rows removed:', rows_before - rows_after)\n",
    "print(df['category'].value_counts())\n",
    "\n",
    "df['token_len'] = df['clean_text'].map(lambda x: len(tokenize(x))).astype(int)\n",
    "summary = df.groupby('category')['token_len'].describe()\n",
    "cols = [c for c in ['count', 'mean', '50%', 'max'] if c in summary.columns]\n",
    "print('Token length summary by class:\\n', summary[cols])\n",
    "\n",
    "X = df['clean_text'].tolist()\n",
    "y = df['category'].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=SEED, stratify=y\n",
    ")\n",
    "\n",
    "print('\\nTrain size:', len(X_train), 'Test size:', len(X_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f76365",
   "metadata": {},
   "source": [
    "## 6. Define 3 feature methods and 3 classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7a9129f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature methods: ['bow_unigram', 'tfidf_unigram', 'ngram_count_1_2']\n",
      "Classifiers: ['decision_tree', 'random_forest', 'knn']\n",
      "Total combinations: 9\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class FeatureSpec:\n",
    "    name: str\n",
    "    vectorizer: ScratchVectorizer\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelSpec:\n",
    "    name: str\n",
    "    estimator: object\n",
    "\n",
    "\n",
    "feature_specs = [\n",
    "    FeatureSpec('bow_unigram', ScratchVectorizer(ngram_range=(1, 1), min_df=2, max_features=10000, use_idf=False)),\n",
    "    FeatureSpec('tfidf_unigram', ScratchVectorizer(ngram_range=(1, 1), min_df=2, max_features=10000, use_idf=True, sublinear_tf=True)),\n",
    "    FeatureSpec('ngram_count_1_2', ScratchVectorizer(ngram_range=(1, 2), min_df=2, max_features=15000, use_idf=False)),\n",
    "]\n",
    "\n",
    "model_specs = [\n",
    "    ModelSpec('decision_tree', DecisionTreeClassifier(max_depth=30, min_samples_split=4, random_state=SEED)),\n",
    "    ModelSpec('random_forest', RandomForestClassifier(n_estimators=200, max_depth=None, min_samples_split=4, random_state=SEED, n_jobs=-1)),\n",
    "    ModelSpec('knn', KNeighborsClassifier(n_neighbors=7, weights='distance', metric='cosine')),\n",
    "]\n",
    "\n",
    "print('Feature methods:', [f.name for f in feature_specs])\n",
    "print('Classifiers:', [m.name for m in model_specs])\n",
    "print('Total combinations:', len(feature_specs) * len(model_specs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8633f8b4",
   "metadata": {},
   "source": [
    "## 7. Train and evaluate all 9 combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1cfcad83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_method</th>\n",
       "      <th>classifier</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_sports</th>\n",
       "      <th>recall_sports</th>\n",
       "      <th>f1_sports</th>\n",
       "      <th>train_time_sec</th>\n",
       "      <th>inference_time_sec</th>\n",
       "      <th>vocab_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ngram_count_1_2</td>\n",
       "      <td>knn</td>\n",
       "      <td>0.967185</td>\n",
       "      <td>0.965157</td>\n",
       "      <td>0.968531</td>\n",
       "      <td>0.966841</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>0.037515</td>\n",
       "      <td>4848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bow_unigram</td>\n",
       "      <td>knn</td>\n",
       "      <td>0.967185</td>\n",
       "      <td>0.968421</td>\n",
       "      <td>0.965035</td>\n",
       "      <td>0.966725</td>\n",
       "      <td>0.001447</td>\n",
       "      <td>0.038846</td>\n",
       "      <td>2975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tfidf_unigram</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.967185</td>\n",
       "      <td>0.989011</td>\n",
       "      <td>0.944056</td>\n",
       "      <td>0.966011</td>\n",
       "      <td>0.268546</td>\n",
       "      <td>0.038501</td>\n",
       "      <td>2975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ngram_count_1_2</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.967185</td>\n",
       "      <td>0.992620</td>\n",
       "      <td>0.940559</td>\n",
       "      <td>0.965889</td>\n",
       "      <td>0.295764</td>\n",
       "      <td>0.037662</td>\n",
       "      <td>4848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tfidf_unigram</td>\n",
       "      <td>knn</td>\n",
       "      <td>0.965458</td>\n",
       "      <td>0.968310</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.001138</td>\n",
       "      <td>0.031375</td>\n",
       "      <td>2975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bow_unigram</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.965458</td>\n",
       "      <td>0.988971</td>\n",
       "      <td>0.940559</td>\n",
       "      <td>0.964158</td>\n",
       "      <td>0.336050</td>\n",
       "      <td>0.037177</td>\n",
       "      <td>2975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bow_unigram</td>\n",
       "      <td>decision_tree</td>\n",
       "      <td>0.898100</td>\n",
       "      <td>0.838806</td>\n",
       "      <td>0.982517</td>\n",
       "      <td>0.904992</td>\n",
       "      <td>0.037542</td>\n",
       "      <td>0.000544</td>\n",
       "      <td>2975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tfidf_unigram</td>\n",
       "      <td>decision_tree</td>\n",
       "      <td>0.898100</td>\n",
       "      <td>0.838806</td>\n",
       "      <td>0.982517</td>\n",
       "      <td>0.904992</td>\n",
       "      <td>0.028635</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>2975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ngram_count_1_2</td>\n",
       "      <td>decision_tree</td>\n",
       "      <td>0.894646</td>\n",
       "      <td>0.833828</td>\n",
       "      <td>0.982517</td>\n",
       "      <td>0.902087</td>\n",
       "      <td>0.039369</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>4848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    feature_method     classifier  accuracy  precision_sports  recall_sports  \\\n",
       "0  ngram_count_1_2            knn  0.967185          0.965157       0.968531   \n",
       "1      bow_unigram            knn  0.967185          0.968421       0.965035   \n",
       "2    tfidf_unigram  random_forest  0.967185          0.989011       0.944056   \n",
       "3  ngram_count_1_2  random_forest  0.967185          0.992620       0.940559   \n",
       "4    tfidf_unigram            knn  0.965458          0.968310       0.961538   \n",
       "5      bow_unigram  random_forest  0.965458          0.988971       0.940559   \n",
       "6      bow_unigram  decision_tree  0.898100          0.838806       0.982517   \n",
       "7    tfidf_unigram  decision_tree  0.898100          0.838806       0.982517   \n",
       "8  ngram_count_1_2  decision_tree  0.894646          0.833828       0.982517   \n",
       "\n",
       "   f1_sports  train_time_sec  inference_time_sec  vocab_size  \n",
       "0   0.966841        0.001332            0.037515        4848  \n",
       "1   0.966725        0.001447            0.038846        2975  \n",
       "2   0.966011        0.268546            0.038501        2975  \n",
       "3   0.965889        0.295764            0.037662        4848  \n",
       "4   0.964912        0.001138            0.031375        2975  \n",
       "5   0.964158        0.336050            0.037177        2975  \n",
       "6   0.904992        0.037542            0.000544        2975  \n",
       "7   0.904992        0.028635            0.000332        2975  \n",
       "8   0.902087        0.039369            0.000534        4848  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def top_terms(model, feature_names, top_k=15):\n",
    "    if len(feature_names) == 0:\n",
    "        return []\n",
    "\n",
    "    if hasattr(model, 'coef_'):\n",
    "        coef = np.asarray(model.coef_)[0]\n",
    "        pos_idx = np.argsort(coef)[-top_k:][::-1]\n",
    "        neg_idx = np.argsort(coef)[:top_k]\n",
    "        rows = []\n",
    "        rows.extend([('sports', feature_names[i], float(coef[i])) for i in pos_idx])\n",
    "        rows.extend([('politics', feature_names[i], float(coef[i])) for i in neg_idx])\n",
    "        return rows\n",
    "\n",
    "    if hasattr(model, 'feature_log_prob_') and hasattr(model, 'classes_'):\n",
    "        classes = list(model.classes_)\n",
    "        if 'sports' in classes and 'politics' in classes:\n",
    "            i_s = classes.index('sports')\n",
    "            i_p = classes.index('politics')\n",
    "            score = model.feature_log_prob_[i_s] - model.feature_log_prob_[i_p]\n",
    "            pos_idx = np.argsort(score)[-top_k:][::-1]\n",
    "            neg_idx = np.argsort(score)[:top_k]\n",
    "            rows = []\n",
    "            rows.extend([('sports', feature_names[i], float(score[i])) for i in pos_idx])\n",
    "            rows.extend([('politics', feature_names[i], float(score[i])) for i in neg_idx])\n",
    "            return rows\n",
    "\n",
    "\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        imp = np.asarray(model.feature_importances_)\n",
    "        idx = np.argsort(imp)[-top_k:][::-1]\n",
    "        return [('global', feature_names[i], float(imp[i])) for i in idx if imp[i] > 0]\n",
    "\n",
    "    return []\n",
    "\n",
    "\n",
    "results = []\n",
    "confusions = {}\n",
    "trained = {}\n",
    "all_top_terms = []\n",
    "misclassified = []\n",
    "\n",
    "for feat in feature_specs:\n",
    "    vec = feat.vectorizer\n",
    "    X_train_vec = vec.fit_transform(X_train)\n",
    "    X_test_vec = vec.transform(X_test)\n",
    "\n",
    "    for mdl in model_specs:\n",
    "        clf = clone(mdl.estimator)\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        clf.fit(X_train_vec, y_train)\n",
    "        train_time = time.perf_counter() - t0\n",
    "\n",
    "        t1 = time.perf_counter()\n",
    "        pred = clf.predict(X_test_vec)\n",
    "        infer_time = time.perf_counter() - t1\n",
    "\n",
    "        acc = accuracy_score(y_test, pred)\n",
    "        p, r, f1, _ = precision_recall_fscore_support(\n",
    "            y_test, pred, average='binary', pos_label='sports', zero_division=0\n",
    "        )\n",
    "\n",
    "        cm = confusion_matrix(y_test, pred, labels=['politics', 'sports'])\n",
    "\n",
    "        row = {\n",
    "            'feature_method': feat.name,\n",
    "            'classifier': mdl.name,\n",
    "            'accuracy': float(acc),\n",
    "            'precision_sports': float(p),\n",
    "            'recall_sports': float(r),\n",
    "            'f1_sports': float(f1),\n",
    "            'train_time_sec': float(train_time),\n",
    "            'inference_time_sec': float(infer_time),\n",
    "            'vocab_size': int(len(vec.vocabulary_)),\n",
    "        }\n",
    "        results.append(row)\n",
    "\n",
    "        key = (feat.name, mdl.name)\n",
    "        confusions[key] = cm\n",
    "        trained[key] = (vec, clf)\n",
    "\n",
    "        for klass, term, score in top_terms(clf, vec.feature_names_, top_k=15):\n",
    "            all_top_terms.append({\n",
    "                'feature_method': feat.name,\n",
    "                'classifier': mdl.name,\n",
    "                'class': klass,\n",
    "                'term': term,\n",
    "                'score': score,\n",
    "            })\n",
    "\n",
    "        for text, gold, guess in zip(X_test, y_test, pred):\n",
    "            if gold != guess:\n",
    "                misclassified.append({\n",
    "                    'feature_method': feat.name,\n",
    "                    'classifier': mdl.name,\n",
    "                    'actual': gold,\n",
    "                    'predicted': guess,\n",
    "                    'headline': text,\n",
    "                })\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(['f1_sports', 'accuracy'], ascending=False).reset_index(drop=True)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc40373c",
   "metadata": {},
   "source": [
    "## 8. Best model details and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f56fbfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best combination: ngram_count_1_2 + knn\n",
      "Accuracy: 0.9672\n",
      "Precision (sports): 0.9652\n",
      "Recall (sports): 0.9685\n",
      "F1 (sports): 0.9668\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_politics</th>\n",
       "      <th>pred_sports</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual_politics</th>\n",
       "      <td>283</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual_sports</th>\n",
       "      <td>9</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 pred_politics  pred_sports\n",
       "actual_politics            283           10\n",
       "actual_sports                9          277"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best = results_df.iloc[0]\n",
    "print('Best combination:', best['feature_method'], '+', best['classifier'])\n",
    "print('Accuracy:', round(best['accuracy'], 4))\n",
    "print('Precision (sports):', round(best['precision_sports'], 4))\n",
    "print('Recall (sports):', round(best['recall_sports'], 4))\n",
    "print('F1 (sports):', round(best['f1_sports'], 4))\n",
    "\n",
    "best_key = (best['feature_method'], best['classifier'])\n",
    "best_cm = confusions[best_key]\n",
    "\n",
    "cm_df = pd.DataFrame(\n",
    "    best_cm,\n",
    "    index=['actual_politics', 'actual_sports'],\n",
    "    columns=['pred_politics', 'pred_sports']\n",
    ")\n",
    "cm_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b3ff26",
   "metadata": {},
   "source": [
    "## 9. Save Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "82a5c035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved output files in: D:\\NLU\\Assn 1\\prob 4\\outputs\n"
     ]
    }
   ],
   "source": [
    "out = Path('outputs')\n",
    "out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "results_df.to_csv(out / 'model_comparison_9combos.csv', index=False)\n",
    "\n",
    "stats = {\n",
    "    'rows_before_cleaning': int(rows_before),\n",
    "    'rows_after_cleaning': int(rows_after),\n",
    "    'rows_removed': int(rows_before - rows_after),\n",
    "    'politics_count_after_cleaning': int((df['category'] == 'politics').sum()),\n",
    "    'sports_count_after_cleaning': int((df['category'] == 'sports').sum()),\n",
    "    'avg_tokens_per_headline': float(df['token_len'].mean()),\n",
    "    'median_tokens_per_headline': float(df['token_len'].median()),\n",
    "}\n",
    "pd.DataFrame(list(stats.items()), columns=['metric', 'value']).to_csv(out / 'dataset_stats_custom.csv', index=False)\n",
    "\n",
    "for (feat_name, model_name), cm in confusions.items():\n",
    "    cm_file = out / f\"cm_{feat_name}__{model_name}.csv\"\n",
    "    pd.DataFrame(cm, index=['politics', 'sports'], columns=['politics', 'sports']).to_csv(cm_file)\n",
    "\n",
    "top_terms_df = pd.DataFrame.from_records(all_top_terms) if all_top_terms else pd.DataFrame(\n",
    "    columns=['feature_method', 'classifier', 'class', 'term', 'score']\n",
    ")\n",
    "top_terms_df.to_csv(out / 'top_terms_all_models.csv', index=False)\n",
    "\n",
    "misclassified_df = pd.DataFrame.from_records(misclassified) if misclassified else pd.DataFrame(\n",
    "    columns=['feature_method', 'classifier', 'actual', 'predicted', 'headline']\n",
    ")\n",
    "misclassified_df.to_csv(out / 'all_misclassifications.csv', index=False)\n",
    "\n",
    "best_summary = pd.DataFrame([\n",
    "    {\n",
    "        'best_feature_method': best['feature_method'],\n",
    "        'best_classifier': best['classifier'],\n",
    "        'accuracy': best['accuracy'],\n",
    "        'precision_sports': best['precision_sports'],\n",
    "        'recall_sports': best['recall_sports'],\n",
    "        'f1_sports': best['f1_sports'],\n",
    "    }           \n",
    "])\n",
    "best_summary.to_csv(out / 'best_model_summary.csv', index=False)\n",
    "\n",
    "best_model_errors = misclassified_df[\n",
    "    (misclassified_df['feature_method'] == best['feature_method']) &\n",
    "    (misclassified_df['classifier'] == best['classifier'])\n",
    "]\n",
    "best_model_errors.to_csv(out / 'best_model_errors.csv', index=False)\n",
    "\n",
    "print('Saved output files in:', out.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff5dec7",
   "metadata": {},
   "source": [
    "## 10. Optional prediction helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "923d7e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: bjp government faces criticism over handling of recent sports events\n",
      "Predicted class: politics\n"
     ]
    }
   ],
   "source": [
    "def predict_headline(text: str):\n",
    "    vec, clf = trained[best_key]\n",
    "    x = vec.transform([normalize_text(text)])\n",
    "    return clf.predict(x)[0]\n",
    "\n",
    "# sample = 'Parliament debate intensifies over new election funding bill'\n",
    "sample = 'bjp government faces criticism over handling of recent sports events'\n",
    "print('Sample:', sample)\n",
    "print('Predicted class:', predict_headline(sample))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f18eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
